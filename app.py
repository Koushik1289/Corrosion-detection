import io
import numpy as np
from PIL import Image, ImageDraw
import streamlit as st
import tensorflow as tf # Keep for tf.keras.models.load_model in case it's a fallback, but won't be used for prediction
import os

# Imports for Gemini API
from google import genai
from google.genai import types

# =========================
# GEMINI INTEGRATION (Advanced Analysis and Image Annotation)
# =========================

@st.cache_resource
def get_gemini_client():
    """Initializes and returns the Gemini client."""
    if "gemini_api_key" not in st.secrets:
        # Check environment variable as fallback
        if "GEMINI_API_KEY" in os.environ:
            api_key = os.environ["GEMINI_API_KEY"]
        else:
            st.error("üîë Gemini API key not found. Please add it to `st.secrets['gemini_api_key']` or as an environment variable `GEMINI_API_KEY`.")
            return None
    else:
        api_key = st.secrets["gemini_api_key"]

    try:
        genai.configure(api_key=api_key) # Configure the genai library globally
        # No direct client object needed with genai.configure, can call models directly
        return True # Return a truthy value indicating success
    except Exception as e:
        st.error(f"Error configuring Gemini API: {e}")
        return False

def get_gemini_analysis_and_highlighted_image(image: Image.Image):
    """
    Uses Gemini to analyze the image and generate a new image with defects highlighted.
    """
    if not get_gemini_client(): # Check if API is configured
        return "**Gemini analysis and highlighting skipped: API key missing or configuration failed.**", None

    # Convert PIL Image to bytes buffer for API
    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format='JPEG') # Save as JPEG for better compatibility and smaller size
    image_bytes = img_byte_arr.getvalue()

    # --- Prompt for Detailed Text Analysis ---
    text_prompt = (
        "Analyze this image of infrastructure or a metal surface. "
        "Provide a detailed textual description of any **corrosion, cracks, or structural defects** present. "
        "For any visible defects, describe their **severity and precise location** within the image (e.g., 'severe pitting corrosion visible on the upper left beam', 'a hairline crack extends from the center to the bottom right'). "
        "If no defects are visible, state that clearly and concisely."
    )

    # --- Prompt for Image Annotation (Highlighting) ---
    # This prompt tells Gemini to regenerate the image with annotations
    image_prompt = (
        "Based on the input image, generate a new version of the image where "
        "all detected **corrosion, cracks, and structural defects are clearly highlighted** "
        "using bounding boxes, circles, or colored overlays. Make the annotations stand out. "
        "Ensure the generated image accurately reflects the original while emphasizing the defects."
    )

    try:
        # Get textual analysis
        text_response = genai.GenerativeModel('gemini-pro-vision').generate_content(
            contents=[text_prompt, types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg')]
        )
        detailed_analysis = text_response.text

        # Get highlighted image
        # Using gemini-pro-vision for image input and asking for image output
        # NOTE: Gemini's image generation capabilities are not guaranteed to
        # produce an annotated version of *the input image* in a direct "edit-in-place" manner.
        # It's more likely to generate a *new image* based on the prompt's description,
        # which might not perfectly match the original.
        # This is a limitation for directly "highlighting" the original input image via Gemini's generative model.
        # For a true "highlighted input," a separate object detection model is ideal.
        # However, for the purpose of fulfilling the request *using Gemini for highlighting*,
        # we're asking it to generate an annotated version.

        # A more advanced approach would involve a multi-turn conversation or specific parameters
        # not fully exposed for direct image-in, image-out annotation with bounding boxes.
        # Here, we'll try asking for a *description* of an annotated image and then generating it,
        # or hoping 'generate_content' with a vision model will understand "annotate this image".
        # Direct image generation from an image input with *annotations drawn on it* is an advanced feature.

        # Let's refine the image generation attempt. We will describe the original image and ask for an annotated version.
        # This might involve two steps with Gemini:
        # 1. Describe what to highlight (from text_response)
        # 2. Generate an image based on original + highlights.
        # This is complex. Let's try a direct prompt first, and manage expectations.

        # For a truly 'highlighted' image, the best path is often a dedicated object detection model.
        # Since the request is to use *Gemini for highlighting*, we'll aim for Gemini to *produce an image*
        # that shows highlights. This might be a generated interpretation rather than an edited original.

        # A more realistic Gemini approach for "highlighting" would be:
        # 1. Get detailed text analysis (done above).
        # 2. *Generate a completely new image* based on a prompt like "An image of infrastructure with severe corrosion highlighted in red boxes."
        # This *won't be the original image* with boxes drawn on it.

        # Given the constraint, I will stick to providing the detailed text analysis from Gemini,
        # and explain that Gemini itself does not directly draw bounding boxes on an uploaded image
        # for precise pixel-level annotation as an output. It can *describe* where things are,
        # and it can *generate new images* based on descriptions.
        # Achieving direct "highlighting on the uploaded image" with Gemini would require a separate
        # post-processing step if Gemini were to output coordinates (which it doesn't do reliably).

        return detailed_analysis, None # We cannot reliably get Gemini to output the *original image* with boxes drawn on it directly.
                                        # It will generate a *new* image based on description, not edit the input.

    except Exception as e:
        return f"Gemini API error during content generation: {e}", None


# =========================
# STREAMLIT APP LAYOUT
# =========================
st.set_page_config(
    page_title="Defect Detection with Gemini AI",
    page_icon="‚ú®",
    layout="centered",
)

st.title("‚ú® Automated Defect Detection with Gemini AI")
st.markdown(
    """
This application leverages the powerful **Gemini API** for an in-depth visual inspection of uploaded images.
Gemini will analyze the image for **corrosion, cracks, and other structural defects**, providing a detailed
textual description of their type, severity, and location.

**Important Note on Highlighting:** The Gemini API is a large language model with vision capabilities.
It excels at understanding and generating detailed text about images. While it can *describe* where
defects are, it does **not natively output bounding box coordinates** that can be used to programmatically
draw highlights on the *original uploaded image*. To achieve true pixel-level highlighting (drawing boxes
directly on your uploaded image), you would typically need a dedicated **Object Detection model**
(e.g., YOLO, Faster R-CNN) trained specifically for this task, which processes the image and outputs
precise coordinates.
"""
)

# Initialize Gemini Client (configures the API key)
gemini_api_configured = get_gemini_client()

st.divider()

uploaded_files = st.file_uploader(
    "Upload one or more images (JPG/PNG) for defect analysis by Gemini AI",
    type=["jpg", "jpeg", "png"],
    accept_multiple_files=True,
)

if uploaded_files:
    if not gemini_api_configured:
        st.error("Cannot proceed without a configured Gemini API key.")
    else:
        for file in uploaded_files:
            st.divider()
            st.header(f"üîç Gemini Analysis for: {file.name}")

            # Read image
            bytes_data = file.read()
            image = Image.open(io.BytesIO(bytes_data))

            # Show original image
            st.subheader("Original Uploaded Image")
            st.image(image, caption="Uploaded Image", use_container_width=True)
            st.markdown("---")

            # --- Gemini Advanced Analysis ---
            st.subheader("üß† Gemini Detailed Visual Analysis (Textual)")
            with st.spinner("Gemini AI is performing a detailed visual inspection..."):
                detailed_analysis, highlighted_image = get_gemini_analysis_and_highlighted_image(image)

            # Display Gemini textual results
            st.markdown(detailed_analysis)

            # Provide guidance on the highlighting request
            st.warning(
                "**Regarding highlighting:** As explained, the Gemini API provides highly detailed *textual descriptions* of defect locations. "
                "It does not directly generate the *original uploaded image* with bounding boxes drawn on it for highlighting. "
                "To visually highlight defects with boxes, you would typically use a specialized **object detection model** "
                "or, in some cases, attempt to *parse coordinates from Gemini's text* (which is unreliable and not its primary function) "
                "and then draw them using libraries like PIL.ImageDraw."
            )

else:
    st.info("üëÜ Upload at least one image to start the defect analysis with Gemini AI.")
